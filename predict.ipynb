{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyf3oA0rJNcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4EOGUeVz68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Support vector regression: The implementation is based on libsvm. The fit time complexity is more than \n",
        "# quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples.\n",
        "# In regression we try to minimise the error rate. While in SVR we try to fit the error within a certain threshold.\n",
        "# Since the value of the RBF (radial basis function) kernel decreases with distance and ranges between zero and one, \n",
        "# it can beinterpreted as a similarity function\n",
        "\n",
        "def reg_SVR(X_train, X_test, y_train):\n",
        "    \n",
        "    clf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
        "    y_pred = clf.fit(X_train,y_train).predict(X_test)\n",
        "    \n",
        "    print('Support vector regression complete.')\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hex4KLcKc7lO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-nearest neighbours regression: The target is predicted by local interpolation of the targets \n",
        "# associated of the nearest neighbors in the training set.\n",
        "# weight = ‘uniform’ : All points in each neighborhood are weighted equally.\n",
        "# weight = ‘distance’ : weight points by the inverse of their distance so closer neighbors\n",
        "# of a query point will have a greater influence than neighbors which are further away.\n",
        "\n",
        "def reg_kNN(X_train, X_test, y_train):\n",
        "    \n",
        "    clf = KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
        "    y_pred = clf.fit(X_train,y_train).predict(X_test)\n",
        "    \n",
        "    print('k-nearest neighbours regression complete.')\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlWNAvnIf5LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multilayer Perceptron: MLPRegressor trains iteratively since at each time step the partial derivatives\n",
        "# of the loss function with respect to the model parameters are computed to update the parameters.\n",
        "# It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting.\n",
        "\n",
        "def reg_MLP(X_train, X_test, y_train):\n",
        "    \n",
        "    clf = MLPRegressor(activation='logistic')\n",
        "    y_pred = clf.fit(X_train,y_train).predict(X_test)\n",
        "    \n",
        "    print('Multilayer perceptron regression complete.')\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8FTa1AXc7zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gradient boosting: n_estimators = number of weak learners, ls is least squares loss function (to be optimized)\n",
        "# It builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable\n",
        "# loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n",
        "\n",
        "def reg_GBR(X_train, X_test, y_train):\n",
        "    \n",
        "    clf = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, loss='ls', random_state=1)\n",
        "    y_pred = clf.fit(X_train,y_train).predict(X_test)\n",
        "    \n",
        "    print('Gradient boosting regression complete.')\n",
        "    \n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}